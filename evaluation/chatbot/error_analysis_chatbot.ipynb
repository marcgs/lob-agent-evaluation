{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de94319",
   "metadata": {},
   "source": [
    "# Chatbot Evaluation Error Analysis\n",
    "\n",
    "This notebook analyzes the results of a chatbot evaluation run. It provides: standard metrics by scenario, function call error analysis, and LLM-based root cause analysis for each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab73378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to evaluation_results.json (update if needed)\n",
    "results_path = Path('evaluation_results.json')\n",
    "with open(results_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "rows = data[0]['rows'] if isinstance(data, list) and 'rows' in data[0] else data\n",
    "df = pd.json_normalize(rows)\n",
    "print(f'Loaded {len(df)} test cases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bc7f",
   "metadata": {},
   "source": [
    "## 1. Standard Metrics by Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0435513",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['outputs.Precision_fn.score', 'outputs.Recall_fn.score', 'outputs.Precision_args.score', 'outputs.Recall_args.score', 'outputs.Reliability.score']\n",
    "scenario_col = 'inputs.scenarioType'\n",
    "summary = df.groupby(scenario_col)[metrics].mean().round(2)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5eefd",
   "metadata": {},
   "source": [
    "## 2. Function Call Error Analysis\n",
    "\n",
    "This section compares expected vs actual function calls and arguments, and identifies which functions fail the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fn_calls(row, key):\n",
    "    calls = row.get(key, [])\n",
    "    if isinstance(calls, str):\n",
    "        try: calls = json.loads(calls)\n",
    "        except: return []\n",
    "    return [c.get('functionName') for c in calls if isinstance(c, dict) and 'functionName' in c]\n",
    "\n",
    "df['expected_fns'] = df.apply(lambda r: extract_fn_calls(r, 'inputs.expected_function_calls'), axis=1)\n",
    "df['actual_fns'] = df.apply(lambda r: extract_fn_calls(r, 'outputs.function_calls'), axis=1)\n",
    "\n",
    "all_expected = [fn for fns in df['expected_fns'] for fn in fns]\n",
    "all_actual = [fn for fns in df['actual_fns'] for fn in fns]\n",
    "expected_counts = Counter(all_expected)\n",
    "actual_counts = Counter(all_actual)\n",
    "\n",
    "print('Expected function call counts:')\n",
    "display(pd.DataFrame.from_dict(expected_counts, orient='index', columns=['expected']))\n",
    "print('Actual function call counts:')\n",
    "display(pd.DataFrame.from_dict(actual_counts, orient='index', columns=['actual']))\n",
    "\n",
    "# Function-level error rates\n",
    "fn_errors = defaultdict(lambda: {'missed': 0, 'extra': 0, 'total': 0})\n",
    "for _, row in df.iterrows():\n",
    "    exp, act = set(row['expected_fns']), set(row['actual_fns'])\n",
    "    for fn in exp - act:\n",
    "        fn_errors[fn]['missed'] += 1\n",
    "    for fn in act - exp:\n",
    "        fn_errors[fn]['extra'] += 1\n",
    "    for fn in exp:\n",
    "        fn_errors[fn]['total'] += 1\n",
    "\n",
    "fn_error_df = pd.DataFrame(fn_errors).T\n",
    "fn_error_df['missed_rate'] = (fn_error_df['missed'] / fn_error_df['total']).round(2)\n",
    "fn_error_df['extra_rate'] = (fn_error_df['extra'] / fn_error_df['total']).round(2)\n",
    "display(fn_error_df.sort_values('missed_rate', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e2655",
   "metadata": {},
   "source": [
    "## 3. LLM-based Root Cause Analysis (per test case)\n",
    "\n",
    "For each test case, we use an LLM to analyze the chat history and function call results to provide insights on root causes of errors. (Requires OpenAI or compatible LLM API key.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412380a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Helper to format chat history for LLM prompt\n",
    "def format_chat_history(chat):\n",
    "    if not isinstance(chat, list):\n",
    "        return str(chat)\n",
    "    out = []\n",
    "    for m in chat:\n",
    "        role = m.get('role', m.get('name', ''))\n",
    "        content = m.get('content', '')\n",
    "        if content:\n",
    "            out.append(f\"{role}: {content}\")\n",
    "    return '\\n'.join(out)\n",
    "\n",
    "def analyze_case_with_llm(chat_history, expected_calls, actual_calls, metrics):\n",
    "    prompt = f'''\n",
    "You are an expert in chatbot evaluation.\n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "Expected function calls: {expected_calls}\n",
    "Actual function calls: {actual_calls}\n",
    "\n",
    "Metrics: {metrics}\n",
    "\n",
    "Analyze the root cause of any errors or mismatches. Provide actionable insights for improvement.\n",
    "'''\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful AI assistant for chatbot evaluation.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=300,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"LLM analysis failed: {e}\"\n",
    "\n",
    "# Run LLM analysis for each test case (limit to first 5 for demo)\n",
    "llm_results = []\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    chat = row.get('outputs.chat_history', [])\n",
    "    expected = row.get('inputs.expected_function_calls', [])\n",
    "    actual = row.get('outputs.function_calls', [])\n",
    "    metrics = {k: row.get(k, None) for k in metrics}\n",
    "    chat_str = format_chat_history(chat)\n",
    "    result = analyze_case_with_llm(chat_str, expected, actual, metrics)\n",
    "    llm_results.append({\n",
    "        'test_case': idx,\n",
    "        'scenario': row.get('inputs.scenarioType', ''),\n",
    "        'llm_analysis': result\n",
    "    })\n",
    "print('LLM root cause analysis complete for first 5 test cases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f11efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llm_df = pd.DataFrame(llm_results)\n",
    "display(llm_df[['test_case', 'scenario', 'llm_analysis']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a17cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "- Place this notebook in the output directory of a chatbot evaluation run (where `evaluation_results.json` is located).\n",
    "- Run all cells to generate error analysis, including LLM-based root cause analysis (requires OpenAI API key).\n",
    "- Review the metrics, function call analysis, and LLM insights to identify areas for improvement in the chatbot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
