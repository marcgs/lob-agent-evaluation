{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read evaluation_results.json\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "file_path = \"evaluation_results.json\"\n",
    "# Read a JSON file and return its content.\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "data = results[0].get(\"rows\", None)\n",
    "# from results extract all k,v pairs without \"rows\" and assing to metrics var\n",
    "metrics = {k: v for k, v in results[0].items() if k != \"rows\"}\n",
    "# Print the metrics\n",
    "print(\"### Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "# Print the number of rows in the data\n",
    "print(\"### Number of rows in data:\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = metrics.keys()\n",
    "metrics_names = [f\"outputs.{name}\" for name in metrics_names]\n",
    "print(\"### Metrics names:\")\n",
    "for name in metrics_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "# Print the first few rows of the DataFrame\n",
    "print(\"### DataFrame:\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa226c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outputs.chat_history\"].apply(lambda x: len(x) if type(x) is list else 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate metrics by scenario type\n",
    "def aggregate_metrics(df):\n",
    "    # Group by scenario_type and calculate the mean for each metric\n",
    "    aggregated_df = df.groupby(\"inputs.scenarioType\")[metrics_names].mean()\n",
    "    return aggregated_df\n",
    "\n",
    "\n",
    "# Aggregate the metrics\n",
    "aggregated_df = aggregate_metrics(df)\n",
    "# Print the aggregated DataFrame\n",
    "print(\"### Aggregated DataFrame:\")\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1882f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data from lowest to highest based on weighted average of metrics\n",
    "# metrics containing \"Recall\" are multiplied by 2\n",
    "# metrics containing \"Reliability\" are multiplied by 3\n",
    "def calculate_weighted_average(row):\n",
    "    weighted_sum = 0\n",
    "    total_weight = 0\n",
    "    for name in metrics_names:\n",
    "        if \"Recall\" in name:\n",
    "            weight = 2\n",
    "        elif \"Reliability\" in name:\n",
    "            weight = 3\n",
    "        else:\n",
    "            weight = 1\n",
    "        value = row[name]\n",
    "        weighted_sum += value * weight\n",
    "        total_weight += weight\n",
    "    return weighted_sum / total_weight if total_weight != 0 else 0\n",
    "\n",
    "\n",
    "# sort the data based on weighted average\n",
    "data.sort(key=calculate_weighted_average)\n",
    "# Print the sorted data\n",
    "print(\"### Sorted data:\")\n",
    "for row in data:\n",
    "    # Print the row with the metrics names\n",
    "    print(\n",
    "        row[\"inputs.scenarioType\"],\n",
    "        {\n",
    "            name.replace(\"outputs.\", \"\").replace(\".score\", \"\"): row[name]\n",
    "            for name in metrics_names\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.chatbot.evaluators.compare import is_similar\n",
    "\n",
    "\n",
    "def compare_fn_details(\n",
    "    function_calls1,\n",
    "    function_calls2,\n",
    "    ignore_functions=[\n",
    "        \"CobraPlugin-get_module_orgs\",\n",
    "        \"CommonPlugin-ask_clarification\",\n",
    "        \"CommonPlugin-start_over\",\n",
    "    ],\n",
    "):\n",
    "    outputs = []\n",
    "    for call1 in function_calls1:\n",
    "        function_name = call1[\"functionName\"]\n",
    "        if function_name in ignore_functions:\n",
    "            continue\n",
    "        # check if functionName is in expected_function_calls\n",
    "        if function_name not in [call[\"functionName\"] for call in function_calls2]:\n",
    "            outputs.append(\n",
    "                f'    - \"{function_name}\" not found in target function calls'\n",
    "            )\n",
    "        else:\n",
    "            args_error_flag = False\n",
    "            # check if arguments are the same\n",
    "            call2 = next(\n",
    "                call\n",
    "                for call in function_calls2\n",
    "                if call[\"functionName\"] == function_name\n",
    "            )\n",
    "            arguments1 = call1[\"arguments\"]\n",
    "            arguments2 = call2[\"arguments\"]\n",
    "            # compare call1 and call2 keys and values wise and print the differences\n",
    "            for key in arguments1.keys():\n",
    "                if key not in arguments2.keys():\n",
    "                    if not args_error_flag:\n",
    "                        args_error_flag = True\n",
    "                        outputs.append(f'    - \"{function_name}\" arguments differ')\n",
    "                    # check argument name level mistakes\n",
    "                    outputs.append(f'      - \"{function_name}\" has extra key {key}')\n",
    "                elif not is_similar(str(arguments1[key]), str(arguments2[key])):\n",
    "                    if not args_error_flag:\n",
    "                        args_error_flag = True\n",
    "                        outputs.append(f'    - \"{function_name}\" arguments differ')\n",
    "                    # check argument value level mistakes\n",
    "                    outputs.append(\n",
    "                        f'      - \"{key}\": \"{arguments1[key]}\" != \"{arguments2[key]}\"'\n",
    "                    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# analyze and display differences in the data by comparing \"outputs.function_calls\" and \"inputs.expected_function_calls\"\n",
    "def compare_function_calls(row):\n",
    "    # Compare the function calls in the row\n",
    "    function_calls = row[\"outputs.function_calls\"]\n",
    "    expected_function_calls = row[\"inputs.expected_function_calls\"]\n",
    "    # Check if they are the same\n",
    "    actual_vs_expected = compare_fn_details(function_calls, expected_function_calls)\n",
    "    expected_vs_actual = compare_fn_details(expected_function_calls, function_calls)\n",
    "    if actual_vs_expected or expected_vs_actual:\n",
    "        print(f'Function calls differ for scenarioType \"{row[\"inputs.scenarioType\"]}\":')\n",
    "        print(\n",
    "            f\"Metrics: {[{name.replace('outputs.', '').replace('.score', ''): row[name]} for name in metrics_names]}\"\n",
    "        )\n",
    "        # Print the differences\n",
    "        if actual_vs_expected:\n",
    "            print(f\"  Actual function calls vs Expected function calls:\")\n",
    "            for line in actual_vs_expected:\n",
    "                print(line)\n",
    "        if expected_vs_actual:\n",
    "            print(f\"  Expected function calls vs Actual function calls:\")\n",
    "            for line in expected_vs_actual:\n",
    "                print(line)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Print the differences for each row\n",
    "print(\"### Function calls differences:\")\n",
    "print(\"====================================\")\n",
    "for row in data:\n",
    "    if compare_function_calls(row):\n",
    "        print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800b014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a8a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648de6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca8cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
